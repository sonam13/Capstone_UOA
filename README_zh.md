
<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> | <a href="https://github.com/RUC-GSAI/YuLan-Mini">English</a> | <a href="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/README_ja.md">æ—¥æœ¬èª</a>
    <p>
</h4>

<div align=center>
<img src="assets/YuLan-logo.jpg" width="400px">
<h1>YuLan-Mini: æ•°æ®é«˜æ•ˆçš„å¼€æºè¯­è¨€æ¨¡å‹</h1>
<a href="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-MIT-blue" alt="license"></a>
<a href="https://arxiv.org/abs/2412.17743" target="_blank"><img src=https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv></a>
<a href="https://huggingface.co/collections/yulan-team/yulan-mini-676d214b24376739b00d95f3"><img alt="Static Badge" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?color=8A2BE2"></a>
<a><img src="https://img.shields.io/github/stars/RUC-GSAI/YuLan-Mini"></a>
</div>

YuLan-Mini æ˜¯ä¸€ä¸ª 2.4B å‚æ•°é‡çš„è½»é‡åŒ–è¯­è¨€æ¨¡å‹ã€‚ä»…ä½¿ç”¨ 1.08T Tokens è¿›è¡Œé¢„è®­ç»ƒï¼Œå´è¾¾åˆ°äº†ä¸ä½¿ç”¨æ›´å¤šæ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯ **æ•°å­¦** å’Œ **ä»£ç ** ä¸¤ä¸ªé¢†åŸŸã€‚ä¸ºæ–¹ä¾¿å¤ç°ï¼Œæˆ‘ä»¬å°†å¼€æºç›¸å…³é¢„è®­ç»ƒèµ„æºã€‚

---

## æ–°é—»

- [2025.01.29] YuLan-Mini-Instruct-v1 å‘å¸ƒ
- [2024.12.23] YuLan-Mini åŠé¢„è®­ç»ƒèµ„æºå‘å¸ƒ

## æ¨¡å‹ä¸‹è½½ ğŸ”—

> YuLan-Mini æ˜¯ [YuLan ç³»åˆ—](https://github.com/RUC-GSAI/YuLan-Chat) çš„ä¸€éƒ¨åˆ†ï¼Œè¯¥ç³»åˆ—è¿˜åŒ…æ‹¬æ›´å¤§è§„æ¨¡å’Œä¸åŒè®­ç»ƒç­–ç•¥çš„æ¨¡å‹ã€‚

|  æ¨¡å‹  | ä¸Šä¸‹æ–‡é•¿åº¦ | SFT | ğŸ¤— Hugging Face | ModelScope | Wise Model |
|---------|----------------|-----|-----------------|------------|------------|
| YuLan-Mini | 28K | â | [`Base`](https://huggingface.co/yulan-team/YuLan-Mini) | [`Base`](https://modelscope.cn/models/yulan-team/YuLan-Mini) | [`Base`](https://wisemodel.cn/models/yulan-team/YuLan-Mini) |
| YuLan-Mini-Instruct | 28K | âœ… | [`Instruct`](https://huggingface.co/yulan-team/YuLan-Mini-Instruct) | | |

> ä¸­é—´æ£€æŸ¥ç‚¹å¯ä»¥åœ¨[è¿™é‡Œ](#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%B5%84%E6%BA%90-)æ‰¾åˆ°ã€‚

---

## èƒ½åŠ›ä»‹ç» ğŸŒŸ

<div align=center>
<img src="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/assets/main.png">
</div>

æˆ‘ä»¬çš„é¢„è®­ç»ƒæ–¹æ³•é€šè¿‡ä»¥ä¸‹ä¸‰é¡¹å…³é”®æŠ€æœ¯æ”¹è¿›æå‡äº†è®­ç»ƒæ•ˆç‡ï¼š

1. ç²¾ç»†çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œå°†æ•°æ®æ¸…æ´—ä¸æ•°æ®è¯¾ç¨‹ç­–ç•¥ç›¸ç»“åˆï¼›
2. ç¨³å®šçš„ä¼˜åŒ–æ–¹æ³•ï¼Œæœ‰æ•ˆç¼“è§£é¢„è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§ï¼›
3. é«˜æ•ˆçš„é€€ç«ç­–ç•¥ï¼Œèåˆäº†ç›®æ ‡æ•°æ®é€‰æ‹©å’Œé•¿ä¸Šä¸‹æ–‡è®­ç»ƒã€‚

æœ€ç»ˆï¼Œä½¿ç”¨æˆ‘ä»¬çš„é«˜æ•ˆé¢„è®­ç»ƒç­–ç•¥ï¼Œä»… 1T çš„æ•°æ®é‡ä¾¿å¯åœ¨æ•°å­¦å’Œä»£ç ç­‰é¢†åŸŸï¼Œåª²ç¾ Qwen2.5-1.5B åœ¨ 18T æ•°æ®ä¸Šçš„æ•ˆæœã€‚æˆ‘ä»¬å°†å¼€æºä½¿ç”¨åˆ°çš„ 1T æ•°æ®ï¼Œå…¶ä¸­æŒ‡ä»¤æ•°æ®ä»…å  3.5%ã€‚

---
## åŸºå‡†æµ‹è¯• ğŸŒŸ

| Models                  | MMLU | CEVAL | GSM8K | ARC_CHALLENGE | GPQA | MATH | HUMANEVAL@1 | MBPP@1 |
|-------------------------|-------|-------|-------|---------------|------|------|-------------|--------|
| Qwen-2.5-1.5B-Instruct  | 57.5  | 65.4  | 73.2  | 47.8          | 29.8 | 55.2 | 61.6        | 88.1   |
| Llama3.2-3B-Instruct    | 60    | 45.9  | 43.4  | 78.6          | 38.6 | 48   | 51.5        | 80.4   |
| YuLan-Mini-Instruct  | 53.6  | 50.5    | 82.3  | 51.8          | 30.1 | 55.2 | 67.7        | 85.7   |

> æ³¨æ„ï¼šæ¨¡å‹å¤§å°çš„è®¡ç®—åŒ…å«äº†åµŒå…¥å±‚ï¼ˆembeddingï¼‰çš„å¤§å°ã€‚

|      Models      | Model Size | # Train Tokens | Context Length | MATH 500 | GSM 8K | Human Eval | MBPP   | RACE Middle | RACE High | RULER  |
|:----------------|----------:|--------------:|--------------:|:--------|:------|:----------|:------|:-----------|:---------|:------|
|     MiniCPM      |    2.6B    |     1.06T      |       4K       |   15.00  |  53.83 |     50.00* |  47.31 |     56.61   |   44.27   |   N/A  |
|      Qwen-2      |    1.5B    |       7T       |      128K      |   22.60  | 46.90* |     34.80* | 46.90* |     55.77   |   43.69   |  60.16 |
|     Qwen2.5      |    0.5B    |      18T       |      128K      |   23.60  | 41.60* |     30.50* | 39.30* |     52.36   |   40.31   |  49.23 |
|     Qwen2.5      |    1.5B    |      18T       |      128K      |   **45.40**  | **68.50\*** |     37.20* | 60.20* |     **58.77**   |   44.33   |  <ins>68.26</ins> |
|     Gemma2       |    2.6B    |       2T       |       8K       |   18.30* | 30.30* |     19.50* | 42.10* |       -     |      -    |   N/A  |
|    StableLM2     |    1.7B    |       2T       |       4K       |     -    |  20.62 |      8.50* |  17.50 |     56.33   |   **45.06**   |   N/A  |
|    SmolLM2       |    1.7B    |      11T       |       8K       |   11.80  |    -   |     23.35  |  45.00 |     55.77   |   43.06   |   N/A  |
|    Llama3.2      |    3.2B    |       9T       |      128K      |    7.40  |    -   |     29.30  |  49.70 |     55.29   |   43.34   |  **77.06** |
|    YuLan-Mini    |    2.4B    |     1.04T      |       4K       |   32.60  |  66.65 |     <ins>61.60</ins>  |  **66.70** |     55.71   |   43.58   |   N/A  |
|    YuLan-Mini    |    2.4B    |     1.08T      |      28K       |  <ins>37.80</ins>  |  <ins>68.46</ins> |    **64.00**  |  <ins>65.90</ins>|     <ins>57.18</ins>   |   <ins>44.57</ins>   |  51.48 |


|      Models      | LAMBADA | MMLU  | CMMLU | CEval | HellaSwag | WinoGrande | StoryCloze | ARC-e | ARC-c |
|:----------------|:-------|:-----|:-----|:-----|:----------|:-----------|:-----------|:-----|:-----|
|   MiniCPM-2.6B   |  61.91  | 53.37 | 48.97 | 48.24 |   67.92    |     65.74   |     78.51   | 55.51 | 43.86 |
|   Qwen2-1.5B     |  64.68  | 55.90 | **70.76** | **71.94** |   66.11    |     66.14   |     77.60   | 62.21 | 42.92 |
|  Qwen2.5-0.5B    |  52.00  | 47.50 | 52.17 | 54.27 |   50.54    |     55.88   |     71.67   | 56.10 | 39.51 |
|  Qwen2.5-1.5B    |  62.12  | <ins>60.71</ins> | <ins>67.82</ins> | <ins>69.05</ins> |   67.18    |     64.48   |     76.80   | **71.51** | <ins>53.41</ins> |
|   Gemma2-2.6B    |    -    | 52.20*|   -   | 28.00*|   <ins>74.60*</ins>   |    **71.50\***   |       -     |   -   | **55.70\***|
| StableLM2-1.7B   |  66.15  | 40.37 | 29.29 | 26.99 |   69.79    |     64.64   |     <ins>78.56</ins>   | 54.00 | 40.78 |
|  SmolLM2-1.7B    |  <ins>67.42</ins>  | 51.91 | 33.46 | 35.10 |   72.96    |     67.40   |     **79.32**   | 44.82 | 35.49 |
|   Llama3.2-3B    |  **69.08**  | **63.40** | 44.44 | 44.49 |   **75.62**    |     <ins>67.48</ins>   |     76.80   | <ins>70.12</ins> | 48.81 |
|    YuLan-Mini    |  64.72  | 51.79 | 48.35 | 51.47 |   68.65    |     67.09   |     76.37   | 69.87 | 50.51 |
|    YuLan-Mini    |  65.67  | 49.10 | 45.45 | 48.23 |   67.22    |     67.24   |     75.89   | 67.47 | 49.32 |

---

## é¢„è®­ç»ƒèµ„æº ğŸ”§

ä¸ºäº†æé«˜ç ”ç©¶çš„é€æ˜åº¦å’Œå¯å¤ç°æ€§ï¼Œæˆ‘ä»¬å¼€æºäº†ç›¸å…³çš„[é¢„è®­ç»ƒèµ„æº](https://github.com/RUC-GSAI/YuLan-Mini/blob/main/pretrain)ï¼š

### é¢„è®­ç»ƒ

<details><summary>1. é¢„è®­ç»ƒå’Œè¯„ä¼°ä»£ç </summary>

é¢„è®­ç»ƒä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/RUC-GSAI/YuLan-Mini/tree/main/pretrain)æ‰¾åˆ°ã€‚è¯·æ³¨æ„ï¼Œç”±äºåç»­çš„ä»£ç ä¿®æ”¹ï¼Œæ­¤ä»£ç å¯èƒ½æ— æ³•ç›´æ¥è¿è¡Œï¼Œå¯èƒ½éœ€è¦è¿›è¡Œä¸€äº›è°ƒæ•´ã€‚

<h4 id="step-1-modify-the-config-json-">æ­¥éª¤ 1ï¼šä¿®æ”¹ <code>config.json</code></h4>
<p>ç”±äº Hugging Face Trainer çš„å®ç°ï¼ŒæŸäº›å‚æ•°å­˜å‚¨åœ¨ <code>config.json</code> æ–‡ä»¶ä¸­ï¼Œæ— æ³•é€šè¿‡ Trainer çš„å‘½ä»¤è¡Œå‚æ•°è¿›è¡Œä¿®æ”¹ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦é¦–å…ˆæ›´æ–° <code>config.json</code> æ–‡ä»¶ä¸­çš„è¿™äº›å‚æ•°ï¼Œç‰¹åˆ«æ˜¯ï¼š</p>
<ul>
<li><strong><code>save_steps</code></strong>ï¼šä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹çš„é¢‘ç‡ã€‚</li>
<li><strong><code>train_batch_size</code></strong>ï¼šæ¯ä¸ª GPU çš„æ‰¹å¤§å°ï¼ˆç›¸å½“äº Trainer ä¸­çš„ <code>per_device_train_batch_size</code>ï¼‰ã€‚åœ¨ç¨³å®šè®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨äº† 1008 çš„æ‰¹å¤§å°ï¼ˆå¤§çº¦ 4M ä¸ª tokenï¼‰ã€‚ä¿æŒç›¸åŒçš„æ‰¹å¤§å°å¯¹äºè®­ç»ƒæ•ˆæœåŒæ ·é‡è¦ã€‚</li>
</ul>
<p>ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ­£ç¡®é…ç½®çš„ <code>config.json</code> æ–‡ä»¶ç¤ºä¾‹ï¼š</p>
<pre><code class="lang-json">{
  <span class="hljs-attr">"best_metric"</span>: <span class="hljs-literal">null</span>,
  <span class="hljs-attr">"best_model_checkpoint"</span>: <span class="hljs-literal">null</span>,
  <span class="hljs-attr">"epoch"</span>: <span class="hljs-number">0.0</span>,
  <span class="hljs-attr">"eval_steps"</span>: <span class="hljs-number">500</span>,
  <span class="hljs-attr">"global_step"</span>: <span class="hljs-number">0</span>,
  <span class="hljs-attr">"is_hyper_param_search"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-attr">"is_local_process_zero"</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-attr">"is_world_process_zero"</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-attr">"log_history"</span>: [],
  <span class="hljs-attr">"logging_steps"</span>: <span class="hljs-number">3</span>,
  <span class="hljs-attr">"max_steps"</span>: <span class="hljs-number">0</span>,
  <span class="hljs-attr">"num_input_tokens_seen"</span>: <span class="hljs-number">0</span>,
  <span class="hljs-attr">"num_train_epochs"</span>: <span class="hljs-number">0</span>,
  <span class="hljs-attr">"save_steps"</span>: <span class="hljs-number">250</span>,
  <span class="hljs-attr">"stateful_callbacks"</span>: {
    <span class="hljs-attr">"TrainerControl"</span>: {
      <span class="hljs-attr">"args"</span>: {
        <span class="hljs-attr">"should_epoch_stop"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-attr">"should_evaluate"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-attr">"should_log"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-attr">"should_save"</span>: <span class="hljs-literal">true</span>,
        <span class="hljs-attr">"should_training_stop"</span>: <span class="hljs-literal">true</span>
      },
      <span class="hljs-attr">"attributes"</span>: {}
    }
  },
  <span class="hljs-attr">"total_flos"</span>: <span class="hljs-number">0</span>,
  <span class="hljs-attr">"train_batch_size"</span>: <span class="hljs-number">3</span>,
  <span class="hljs-attr">"trial_name"</span>: <span class="hljs-literal">null</span>,
  <span class="hljs-attr">"trial_params"</span>: <span class="hljs-literal">null</span>
}
</code></pre>
<h4 id="step-2-enable-universal-checkpointing-in-the-deepspeed-configuration">æ­¥éª¤ 2ï¼šåœ¨ DeepSpeed é…ç½®ä¸­å¯ç”¨é€šç”¨æ£€æŸ¥ç‚¹</h4>
<p>ä¸ºäº†ç¡®ä¿ DeepSpeed é›†æˆåŠ è½½é€šç”¨æ£€æŸ¥ç‚¹ï¼Œæ‚¨éœ€è¦åœ¨ DeepSpeed é…ç½® JSON æ–‡ä»¶ä¸­å¯ç”¨æ­¤åŠŸèƒ½ã€‚</p>
<p>ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¯ç”¨äº†é€šç”¨æ£€æŸ¥ç‚¹çš„ ZeRO2 é…ç½®ç¤ºä¾‹ï¼š</p>
<pre><code class="lang-json">{
  <span class="hljs-attr">"bf16"</span>: {
    <span class="hljs-attr">"enabled"</span>: <span class="hljs-string">"auto"</span>
  },
  <span class="hljs-attr">"zero_optimization"</span>: {
    <span class="hljs-attr">"stage"</span>: <span class="hljs-number">2</span>,
    <span class="hljs-attr">"allgather_partitions"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-attr">"allgather_bucket_size"</span>: <span class="hljs-number">8e8</span>,
    <span class="hljs-attr">"overlap_comm"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-attr">"reduce_scatter"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-attr">"reduce_bucket_size"</span>: <span class="hljs-number">8e8</span>,
    <span class="hljs-attr">"contiguous_gradients"</span>: <span class="hljs-literal">true</span>
  },
  <span class="hljs-attr">"gradient_accumulation_steps"</span>: <span class="hljs-string">"auto"</span>,
  <span class="hljs-attr">"gradient_clipping"</span>: <span class="hljs-string">"auto"</span>,
  <span class="hljs-attr">"steps_per_print"</span>: <span class="hljs-number">16</span>,
  <span class="hljs-attr">"train_batch_size"</span>: <span class="hljs-string">"auto"</span>,
  <span class="hljs-attr">"train_micro_batch_size_per_gpu"</span>: <span class="hljs-string">"auto"</span>,
  <span class="hljs-attr">"wall_clock_breakdown"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-attr">"dump_state"</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-attr">"optimizer"</span>: {
    <span class="hljs-attr">"type"</span>: <span class="hljs-string">"AdamW"</span>,
    <span class="hljs-attr">"params"</span>: {
      <span class="hljs-attr">"lr"</span>: <span class="hljs-string">"auto"</span>,
      <span class="hljs-attr">"betas"</span>: <span class="hljs-string">"auto"</span>,
      <span class="hljs-attr">"eps"</span>: <span class="hljs-string">"auto"</span>,
      <span class="hljs-attr">"weight_decay"</span>: <span class="hljs-string">"auto"</span>
    }
  },
  <span class="hljs-attr">"checkpoint"</span>: {
    <span class="hljs-attr">"load_universal"</span>: <span class="hljs-literal">true</span>
  }
}
</code></pre>
<h4 id="step-3-resume-training">æ­¥éª¤ 3ï¼šæ¢å¤è®­ç»ƒ</h4>
<p>è°ƒç”¨ <code>trainer.train</code> æ—¶ï¼ŒåŒ…å« <code>resume_from_checkpoint</code> å‚æ•°ä»¥ä»é€šç”¨æ£€æŸ¥ç‚¹åŠ è½½åˆ†å¸ƒå¼ä¼˜åŒ–å™¨çŠ¶æ€å¹¶æ¢å¤è®­ç»ƒã€‚</p>
<pre><code class="lang-python"><span class="hljs-attr">trainer.train(resume_from_checkpoint</span>=<span class="hljs-string">training_args.resume_from_checkpoint)</span>
</code></pre>
<p>æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå†…éƒ¨<a href="https://github.com/RUC-GSAI/YuLan-Mini/tree/main/pretrain">è®­ç»ƒæ¡†æ¶</a>ä¾›æ‚¨å‚è€ƒï¼Œä½†æ‚¨å¯ä»¥è‡ªç”±é€‰æ‹©å…¶ä»–æ¡†æ¶ã€‚</p>

</details>

<details><summary>2. ä¸­é—´é˜¶æ®µæ£€æŸ¥ç‚¹</summary>
ä¸­é—´é˜¶æ®µæ£€æŸ¥ç‚¹å‘å¸ƒåœ¨ <a href="https://huggingface.co/collections/yulan-team/yulan-mini-676d214b24376739b00d95f3">YuLan-Mini</a> ä¸­ã€‚

<table>
    <thead>
        <tr>
            <th>é˜¶æ®µ</th>
            <th>è¯¾ç¨‹é˜¶æ®µ</th>
            <th>4K ä¸Šä¸‹æ–‡</th>
            <th>28K ä¸Šä¸‹æ–‡</th>
            <th>ä¼˜åŒ–å™¨</th>
            <th>æ¨ç†æ¶æ„</th>
            <th>LAMBADA <code>Acc</code></th>
            <th>GSM8K <code>Acc</code></th>
            <th>HumanEval <code>pass@1</code></th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>ç¨³å®š</td>
            <td>5</td>
            <td><a href="https://huggingface.co/yulan-team/YuLan-Mini-Phase5">YuLan-Mini-Phase5</a></td>
            <td></td>
            <td></td>
            <td><code>yulanmini</code></td>
            <td>53.85</td>
            <td>3.41</td>
            <td>12.26</td>
        </tr>
        <tr>
            <td>ç¨³å®š</td>
            <td>10</td>
            <td><a href="https://huggingface.co/yulan-team/YuLan-Mini-Phase10">YuLan-Mini-Phase10</a></td>
            <td></td>
            <td></td>
            <td><code>yulanmini</code></td>
            <td>55.00</td>
            <td>9.57</td>
            <td>15.95</td>
        </tr>
        <tr>
            <td>ç¨³å®š</td>
            <td>15</td>
            <td><a href="https://huggingface.co/yulan-team/YuLan-Mini-Phase15">YuLan-Mini-Phase15</a></td>
            <td></td>
            <td></td>
            <td><code>yulanmini</code></td>
            <td>55.81</td>
            <td>13.81</td>
            <td>16.99</td>
        </tr>
        <tr>
            <td>ç¨³å®š</td>
            <td>20</td>
            <td><a href="https://huggingface.co/yulan-team/YuLan-Mini-Phase20">YuLan-Mini-Phase20</a></td>
            <td></td>
            <td>âœ…</td>
            <td><code>yulanmini</code></td>
            <td>55.81</td>
            <td>21.39</td>
            <td>20.79</td>
        </tr>
        <tr>
            <td>ç¨³å®š</td>
            <td>25 (1T tokens)</td>
            <td><a href="https://huggingface.co/yulan-team/YuLan-Mini-Before-Annealing">YuLan-Mini-Before-Annealing</a></td>
            <td></td>
            <td>âœ…</td>
            <td><code>yulanmini</code></td>
            <td>55.67</td>
            <td>29.94</td>
            <td>34.06</td>
        </tr>
        <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>é€€ç«</td>
            <td>26</td>
            <td>YuLan-Mini-4K</td>
            <td></td>
            <td></td>
            <td><code>llama</code>*</td>
            <td>64.72</td>
            <td>66.65</td>
            <td>61.60</td>
        </tr>
        <tr>
            <td>é€€ç«</td>
            <td>27</td>
            <td></td>
            <td><a href="https://huggingface.co/yulan-team/YuLan-Mini">YuLan-Mini</a></td>
            <td></td>
            <td><code>llama</code>*</td>
            <td>65.67</td>
            <td>68.46</td>
            <td>64.00</td>
        </tr>
    </tbody>
</table>

\*ï¼šä¸ºäº†æ›´å®¹æ˜“æ¨ç†å’Œéƒ¨ç½²ï¼Œæˆ‘ä»¬å°†é‡æ–°å‚æ•°åŒ–çš„é™„åŠ å‚æ•°å’Œç¼©æ”¾å› å­åˆå¹¶åˆ°æœ€ç»ˆå‘å¸ƒçš„æ¨¡å‹ä¸­ ([**YuLan-Mini**](https://huggingface.co/yulan-team/YuLan-Mini) å’Œ **YuLan-Mini-Intermediate-4K**)ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ Llama æ¶æ„ä¸Šè¿è¡Œã€‚ä½†æ˜¯ï¼Œè¿™äº›å‚æ•°ä»ç„¶ä¿ç•™åœ¨è®­ç»ƒè¿‡ç¨‹çš„ä¸­é—´æ£€æŸ¥ç‚¹ä¸­ã€‚

</details>

<details><summary>3. é€€ç«å‰çš„ä¼˜åŒ–å™¨çŠ¶æ€</summary>

<a href="https://huggingface.co/yulan-team/YuLan-Mini-Before-Annealing">ğŸ¤— YuLan-Mini-Before-Annealing</a>
</details>

### æ•°æ®é›†


<details><summary>4. ä½¿ç”¨çš„å¼€æºæ•°æ®é›†</summary>

<a href="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/pretrain/datasets">ä½¿ç”¨çš„å¼€æºæ•°æ®é›†åˆ—è¡¨</a>

</details>

<details><summary>5. æ¯ä¸ªé˜¶æ®µçš„æ•°æ®åˆ†å¸ƒ</summary>

â¬‡ï¸ ç‚¹å‡»æŸ¥çœ‹æ›´å¤šè¯¦æƒ…ï¼š
<a href="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/pretrain/datasets/final.pdf">
  <div align=center>
    <img src="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/assets/data_distribution_for_every_phase.png">
  </div>
</a>

</details>

<details><summary>6. åˆæˆæ•°æ®</summary>

<a href="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/pretrain/preprocess">æ•°æ®æ¸…æ´—</a> å’Œ <a href="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/pretrain/synthesis">åˆæˆ</a> æµç¨‹ï¼š

<div align=center>
<img src="https://github.com/RUC-GSAI/YuLan-Mini/blob/main/assets/data-pipeline.png">
</div>

æˆ‘ä»¬ä½¿ç”¨çš„åˆæˆæ•°æ®å‘å¸ƒåœ¨ <a href="https://huggingface.co/collections/yulan-team/yulan-mini-676d214b24376739b00d95f3">ğŸ¤— YuLan-Mini-Datasets</a>

</details>


### æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›é¢„è®­ç»ƒèµ„æºåšä»€ä¹ˆ

1. **é¢„è®­ç»ƒ**æ‚¨è‡ªå·±çš„ LLMã€‚æ‚¨å¯ä»¥ä½¿ç”¨[æˆ‘ä»¬çš„æ•°æ®](https://huggingface.co/yulan-team/YuLan-Mini-Datasets)å’Œè¯¾ç¨‹æ¥è®­ç»ƒä¸€ä¸ªä¸ YuLan-Mini ä¸€æ ·å¼ºå¤§çš„æ¨¡å‹ã€‚
2. æ‰§è¡Œæ‚¨è‡ªå·±çš„**å­¦ä¹ ç‡é€€ç«**ã€‚åœ¨é€€ç«é˜¶æ®µï¼ŒYuLan-Mini çš„å­¦ä¹ èƒ½åŠ›è¾¾åˆ°é¡¶å³°ã€‚æ‚¨å¯ä»¥ä»[é€€ç«å‰çš„æ£€æŸ¥ç‚¹](https://huggingface.co/yulan-team/YuLan-Mini-Before-Annealing)æ¢å¤è®­ç»ƒï¼Œå¹¶ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œå­¦ä¹ ç‡é€€ç«ã€‚
3. **å¾®è°ƒ** LLM çš„ Instruct ç‰ˆæœ¬ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ [YuLan-Mini](https://huggingface.co/yulan-team/YuLan-Mini) åŸºç¡€æ¨¡å‹æ¥è®­ç»ƒæ‚¨è‡ªå·±çš„ Instruct ç‰ˆæœ¬ã€‚
4. **è®­ç»ƒåŠ¨æ€**ç ”ç©¶ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ YuLan-Mini çš„[ä¸­é—´æ£€æŸ¥ç‚¹](https://huggingface.co/collections/yulan-team/yulan-mini-676d214b24376739b00d95f3)æ¥æ¢ç´¢é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…éƒ¨å˜åŒ–ã€‚
5. **åˆæˆ**æ‚¨è‡ªå·±çš„æ•°æ®ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ YuLan-Mini çš„[æ•°æ®æµç¨‹](https://github.com/RUC-GSAI/YuLan-Mini)æ¥æ¸…ç†å’Œç”Ÿæˆæ‚¨è‡ªå·±çš„æ•°æ®é›†ã€‚
---

## å¿«é€Ÿå¼€å§‹ ğŸ’»

ä»¥ä¸‹æ˜¯ä½¿ç”¨ Huggingface çš„ç®€å•æ¨ç†ä»£ç ç¤ºä¾‹ï¼š

**Huggingface æ¨ç†ç¤ºä¾‹**
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("yulan-team/YuLan-Mini-Instruct")
model = AutoModelForCausalLM.from_pretrained("yulan-team/YuLan-Mini-Instruct", torch_dtype=torch.bfloat16)

# Input text
chat = [
    {"role": "system", "content": "You are YuLan-Mini, created by RUC AI Box. You are a helpful assistant."},
    {"role": "user", "content": "What is Renmin University of China?"}
]
formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(formatted_chat, return_tensors="pt", add_special_tokens=False)

# Completion
output = model.generate(inputs["input_ids"], max_new_tokens=100, temperature=0.5)
print(tokenizer.decode(output[0][inputs['input_ids'].size(1):], skip_special_tokens=True))
```

**vLLMéƒ¨ç½²ç¤ºä¾‹**
```bash
vllm serve yulan-team/YuLan-Mini-Instruct --dtype bfloat16
```

**SGLangéƒ¨ç½²ç¤ºä¾‹**
```bash
python -m sglang.launch_server --model-path yulan-team/YuLan-Mini-Instruct --port 30000 --host 0.0.0.0
```

**Ollamaéƒ¨ç½²ç¤ºä¾‹**
```bash
ollama run hf.co/mradermacher/YuLan-Mini-Instruct-GGUF:IQ4_XS
```

---

## è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼ŒåŒ…æ‹¬æ¨¡å‹é”™è¯¯æ¡ˆä¾‹çš„åé¦ˆã€åŠŸèƒ½å»ºè®®å’Œç¤ºä¾‹è´¡çŒ®ã€‚æ‚¨å¯ä»¥é€šè¿‡æäº¤[issue](https://github.com/RUC-GSAI/YuLan-Mini/issues)æ¥è´¡çŒ®ã€‚

## è®¸å¯åè®®

- æœ¬ä»“åº“ä»£ç ä½¿ç”¨ [MIT License](./LICENSE)ã€‚
- å±€é™æ€§ï¼šå°½ç®¡æˆ‘ä»¬å°è¯•å‡å°‘æ¨¡å‹åœ¨ä½¿ç”¨ä¸­å¯èƒ½å‡ºç°çš„å®‰å…¨æ€§é—®é¢˜ï¼Œå¹¶é¼“åŠ±æ¨¡å‹ç”Ÿæˆç¬¦åˆé“å¾·å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†ç”±äºè¯­è¨€æ¨¡å‹åŸºäºæ¦‚ç‡ç”Ÿæˆçš„èŒƒå¼ï¼Œæ¨¡å‹ä»ç„¶å¯èƒ½ä¼šäº§ç”Ÿæ„å¤–çš„è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œç”Ÿæˆçš„å“åº”å¯èƒ½åŒ…å«åè§ã€æ­§è§†æˆ–å…¶ä»–æœ‰å®³å†…å®¹ã€‚è¯·ä¸è¦ä¼ æ’­æ­¤ç±»å†…å®¹ã€‚æˆ‘ä»¬å¯¹å› ä¼ æ’­æœ‰å®³ä¿¡æ¯è€Œé€ æˆçš„ä»»ä½•åæœä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨å‘ç° YuLan-Mini å¯¹æ‚¨çš„ç ”ç©¶æˆ–å¼€å‘æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2412.17743)ï¼š

```BibTex
@article{hu2024yulan,
  title={YuLan-Mini: An Open Data-efficient Language Model},
  author={Hu, Yiwen and Song, Huatong and Deng, Jia and Wang, Jiapeng and Chen, Jie and Zhou, Kun and Zhu, Yutao and Jiang, Jinhao and Dong, Zican and Zhao, Wayne Xin and others},
  journal={arXiv preprint arXiv:2412.17743},
  year={2024}
}
```
